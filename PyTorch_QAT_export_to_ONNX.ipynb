{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4868ce10",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial demonstrates how to perform quantization aware training (QAT) on a [DistilBERT](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model and export the quantized PyTorch model to an onnx model.\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "### 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets neural-compressor transformers torch onnxruntime onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee259d4a",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f90808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "task = \"sst2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a5fe6",
   "metadata": {},
   "source": [
    "### 1. Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5177ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/13/2023 16:39:18 - WARNING - datasets.builder -   Found cached dataset glue (/home/yuwenzho/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "02/13/2023 16:39:18 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/yuwenzho/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-12eb82fe51b10b17.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARN)\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "class GLUEDataset:\n",
    "    def __init__(self, task, model_name_or_path, max_seq_length=128, data_dir=None):\n",
    "        raw_dataset = load_dataset('glue', task, cache_dir=data_dir, split='train')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        sentence1_key, sentence2_key = task_to_keys[task]\n",
    "        origin_keys = raw_dataset[0].keys()\n",
    "        def preprocess_function(examples):\n",
    "            # Tokenize the texts\n",
    "            args = (\n",
    "                (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "            )\n",
    "            result = tokenizer(*args, padding=True, max_length=max_seq_length, truncation=True)\n",
    "            if  \"label\" in examples:\n",
    "                result[\"label\"] = examples[\"label\"]\n",
    "            return result\n",
    "        self.dataset = raw_dataset.map(\n",
    "            preprocess_function, batched=True, load_from_cache_file=True, remove_columns=origin_keys\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.dataset[index]\n",
    "        return batch\n",
    "\n",
    "# Generate SST-2 dataloader for DistilBERT model\n",
    "dataset = GLUEDataset(task=task, \n",
    "                      model_name_or_path=model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3a094",
   "metadata": {},
   "source": [
    "### 2. Perform quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c912f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 16:39:23 [WARNING] Force convert framework model to neural_compressor model.\n",
      "2023-02-13 16:39:23 [INFO] Fx trace of the entire model failed. We will conduct auto quantization\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "2023-02-13 16:39:29 [INFO] |******Mixed Precision Statistics******|\n",
      "2023-02-13 16:39:29 [INFO] +----------------------+-------+-------+\n",
      "2023-02-13 16:39:29 [INFO] |       Op Type        | Total |  INT8 |\n",
      "2023-02-13 16:39:29 [INFO] +----------------------+-------+-------+\n",
      "2023-02-13 16:39:29 [INFO] |      Embedding       |   2   |   2   |\n",
      "2023-02-13 16:39:29 [INFO] | quantize_per_tensor  |   51  |   51  |\n",
      "2023-02-13 16:39:29 [INFO] |      LayerNorm       |   13  |   13  |\n",
      "2023-02-13 16:39:29 [INFO] |      dequantize      |   51  |   51  |\n",
      "2023-02-13 16:39:29 [INFO] |        Linear        |   38  |   38  |\n",
      "2023-02-13 16:39:29 [INFO] |       Dropout        |   6   |   6   |\n",
      "2023-02-13 16:39:29 [INFO] +----------------------+-------+-------+\n",
      "2023-02-13 16:39:29 [INFO] Training finished!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from neural_compressor import QuantizationAwareTrainingConfig\n",
    "from neural_compressor.training import prepare_compression\n",
    "\n",
    "# training function\n",
    "def train_func(compression_manager, model, dataloader):\n",
    "    compression_manager.callbacks.on_train_begin()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    epochs = 1\n",
    "    iters = 10\n",
    "    for nepoch in range(epochs):\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            batch.pop('labels')\n",
    "            output = model(**batch)\n",
    "            loss = output.logits[0][0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if idx >= iters:\n",
    "                break\n",
    "    compression_manager.callbacks.on_train_end()\n",
    "    return model\n",
    "\n",
    "# Perform quantization aware training\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "quant_conf = QuantizationAwareTrainingConfig()\n",
    "compression_manager = prepare_compression(model, quant_conf)\n",
    "q_model = train_func(compression_manager, compression_manager.model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcc8c1",
   "metadata": {},
   "source": [
    "### 3. Export to ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b027c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuwenzho/miniconda3/envs/jupyter/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:218: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "2023-02-13 16:39:35 [INFO] Weight type: QInt8.\n",
      "2023-02-13 16:39:35 [INFO] Activation type: QUInt8.\n",
      "02/13/2023 16:39:51 - WARNING - root -   Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "02/13/2023 16:40:04 - WARNING - root -   Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "2023-02-13 16:40:05 [INFO] *************************************************************************************************\n",
      "2023-02-13 16:40:05 [INFO] The INT8 ONNX Model is exported to path: distilbert-base-uncased-finetuned-sst-2-english-qat.onnx\n",
      "2023-02-13 16:40:05 [INFO] *************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Get params for export function\n",
    "it = iter(dataloader)\n",
    "input = next(it)\n",
    "input.pop('labels')\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "dynamic_axes = {k: symbolic_names for k in input.keys()}\n",
    "\n",
    "# Export INT8 PyTorch model to INT8 ONNX model\n",
    "from neural_compressor.config import Torch2ONNXConfig\n",
    "int8_onnx_config = Torch2ONNXConfig(dtype=\"int8\",\n",
    "                                    opset_version=14,\n",
    "                                    quant_format=\"QLinear\", # or \"QDQ\" to export to QDQ ONNX model\n",
    "                                    example_inputs=tuple(input.values()),\n",
    "                                    input_names=list(input.keys()),\n",
    "                                    output_names=['labels'],\n",
    "                                    dynamic_axes=dynamic_axes,\n",
    "                                    )\n",
    "q_model.export('distilbert-base-uncased-finetuned-sst-2-english-qat.onnx', int8_onnx_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d461d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
